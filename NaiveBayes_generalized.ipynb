{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "    for token in nltk.word_tokenize(text): #word_tokenize() to split a sentence into words.\n",
    "        if token in string.punctuation: continue # puncutation are '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        #yield stem.stem(token)     # here yield command i sused as length of array for tokem will be huge\n",
    "        yield token                 #slighy higher perfomance\n",
    "def tokenize2(text):    \n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',test,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case    \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rmallik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset,labels): \n",
    "    if not isinstance(dataset,np.ndarray): dataset=np.array(dataset)\n",
    "    if not isinstance(labels,np.ndarray): labels=np.array(labels)\n",
    "    class_list= np.unique(labels)\n",
    "    bow_dicts=np.array([defaultdict(lambda:0) for index in range(len(class_list))]) \n",
    "    word_count_class=[]\n",
    "    prob_class=[]\n",
    "    total_voc=[]\n",
    "    for i,class_i in enumerate(class_list):\n",
    "        data_class=dataset[labels==class_i]\n",
    "        data_class = [list(tokenize(doc)) for doc in data_class] \n",
    "        word_class=[word for example in data_class for word in example]\n",
    "        #total word count in class\n",
    "        word_count_class.append(len(word_class))\n",
    "        # BOW of a class: (word:count_word)\n",
    "        for word in word_class:\n",
    "            bow_dicts[i][word]+=1 \n",
    "        # for vocabulraty count \n",
    "        total_voc+=word_class        \n",
    "        #prob of class \n",
    "        prob_class.append(len(labels[labels==class_i])/len(labels))\n",
    "    dataset_class=[]  \n",
    "    for i,class_i in enumerate(class_list):        \n",
    "        dataset_class.append([bow_dicts[i],prob_class[i],word_count_class[i]+len(list(set(total_voc)))+1])\n",
    "    return dataset_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_prob(train_data,example,classes):\n",
    "    likelihood_class=np.zeros(len(classes))\n",
    "    for i,class_i in enumerate(classes):\n",
    "        for token in example:\n",
    "            token_prob=np.log((train_data[i][0].get(token,0) + 1)/ train_data[i][2])\n",
    "            likelihood_class[i]+=token_prob\n",
    "    return likelihood_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_data,dataset,labels):\n",
    "    class_list=np.unique(labels)\n",
    "    #clean\n",
    "    dataset= [list(tokenize(examples)) for examples in dataset]\n",
    "    \n",
    "    prediction=[]\n",
    "    predict_class=np.zeros(len(class_list))\n",
    "    for data in dataset:\n",
    "        #predict the\n",
    "        posterior_prob=np.zeros(len(class_list))\n",
    "        lkl_prob=likelihood_prob(train_data,data,class_list) \n",
    "        for i , val in enumerate(lkl_prob):\n",
    "            posterior_prob[i]=lkl_prob[i]+ np.log(train_data[i][1])        \n",
    "        predict_class_arg=np.argmax(posterior_prob)\n",
    "        prediction.append(class_list[predict_class_arg])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----prepare data and train and test and find accuarcy as shown\n",
    "#train_data_result=train(dataset,labels)\n",
    "#prediction=test(train_data_result,dataset,labels)\n",
    "#accuracy = accuracy_score(prediction,list(labels_test))\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on fetch_20newsgroups data \n",
    "note:fetch_20newsgroupsis a dataset that has 20 categories but we will restrict the categories\n",
    "to 4 for the time being "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med'] \n",
    "newsgroups_train=fetch_20newsgroups(subset='train',categories=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Training Examples:  2257\n",
      "Total Number of Training Labels:  2257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=newsgroups_train.data #getting all trainign examples\n",
    "train_labels=newsgroups_train.target #getting training labels\n",
    "print (\"Total Number of Training Examples: \",len(train_data)) # Outputs -> Total Number of Training Examples:  2257\n",
    "print (\"Total Number of Training Labels: \",len(train_labels)) # Outputs -> #Total Number of Training Labels:  2257\n",
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_info=train(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test Examples:  1502\n",
      "Number of Test Labels:  1502\n"
     ]
    }
   ],
   "source": [
    "newsgroups_test=fetch_20newsgroups(subset='test',categories=categories) #loading test data\n",
    "test_data=newsgroups_test.data #get test set examples\n",
    "test_labels=newsgroups_test.target #get test set labels\n",
    "\n",
    "print (\"Number of Test Examples: \",len(test_data)) # Output : Number of Test Examples:  1502\n",
    "print (\"Number of Test Labels: \",len(test_labels)) # Output : Number of Test Labels:  1502\n",
    "prediction=test(train_data_info,test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9254327563249002"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(prediction,list(test_labels))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"From: brian@ucsd.edu (Brian Kantor)\\nSubject: Re: HELP for Kidney Stones ..............\\nOrganization: The Avant-Garde of the Now, Ltd.\\nLines: 12\\nNNTP-Posting-Host: ucsd.edu\\n\\nAs I recall from my bout with kidney stones, there isn't any\\nmedication that can do anything about them except relieve the pain.\\n\\nEither they pass, or they have to be broken up with sound, or they have\\nto be extracted surgically.\\n\\nWhen I was in, the X-ray tech happened to mention that she'd had kidney\\nstones and children, and the childbirth hurt less.\\n\\nDemerol worked, although I nearly got arrested on my way home when I barfed\\nall over the police car parked just outside the ER.\\n\\t- Brian\\n\",\n",
       " 2,\n",
       " 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_data[0],test_labels[0],prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis on Large Movie Review Dataset v1.0\n",
    "\n",
    "available at http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down load data and extract it, we are going to create the BOW feature instead of directly using the one given in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "neg_corpus=[]\n",
    "directory=r'C:\\Users\\rmallik\\Documents\\Learn\\ML notes-20190723T064757Z-001\\Assignment\\aclImdb\\train\\neg'\n",
    "for filename in os.listdir(directory):\n",
    "    filename2=os.path.join(directory,filename)\n",
    "    with open(filename2, 'r',encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        neg_corpus.append(text)\n",
    "pos_corpus=[]\n",
    "directory=r'C:\\Users\\rmallik\\Documents\\Learn\\ML notes-20190723T064757Z-001\\Assignment\\aclImdb\\train\\pos'\n",
    "for filename in os.listdir(directory):\n",
    "    filename2=os.path.join(directory,filename)\n",
    "    with open(filename2, 'r',encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        pos_corpus.append(text)\n",
    "corpus=pos_corpus+neg_corpus\n",
    "dataset=np.array(corpus)\n",
    "labels=list(1 * np.ones(len(pos_corpus)))+ list(-1* np.ones(len(neg_corpus)))\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test  files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "pos_corpus_test=[]\n",
    "directory=r'C:\\Users\\rmallik\\Documents\\Learn\\ML notes-20190723T064757Z-001\\Assignment\\aclImdb\\test\\pos'\n",
    "for filename in os.listdir(directory):\n",
    "    filename2=os.path.join(directory,filename)\n",
    "    with open(filename2, 'r',encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        pos_corpus_test.append(text)\n",
    "neg_corpus_test=[]\n",
    "directory=r'C:\\Users\\rmallik\\Documents\\Learn\\ML notes-20190723T064757Z-001\\Assignment\\aclImdb\\test\\neg'\n",
    "for filename in os.listdir(directory):\n",
    "    filename2=os.path.join(directory,filename)\n",
    "    with open(filename2, 'r',encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        neg_corpus_test.append(text)\n",
    "corpus_test=pos_corpus_test+neg_corpus_test\n",
    "dataset_test=np.array(corpus_test)\n",
    "labels_test=list(1 * np.ones(len(pos_corpus)))+ list(-1* np.ones(len(neg_corpus)))\n",
    "labels_test=np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_info=train(dataset,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=test(train_data_info,dataset_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81184"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(prediction,list(labels_test))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[10973  1527]\n",
      " [ 3177  9323]]\n"
     ]
    }
   ],
   "source": [
    "results = confusion_matrix(list(labels_test), prediction) \n",
    "print('Confusion Matrix :')\n",
    "print(results) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
